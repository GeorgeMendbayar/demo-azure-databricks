{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcddca6c-5633-49c1-b4f0-f6d3958c7f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from ortools.linear_solver import pywraplp\n",
    "from copy import deepcopy\n",
    "from math import isclose\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import truediv as divide\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "site_alias = {\n",
    "    'southernops_vc': {\n",
    "        'amrun_loadandhaul': 'p1',\n",
    "        'amrun_rom_stp': 's1',\n",
    "        'amrun_bene': 'p2',\n",
    "        'amrun_p_stp': 's2',\n",
    "        'amrun_shiploader': 'p3'\n",
    "    },\n",
    "    'gove_vc': {\n",
    "        'gove_loadandhaul': 'p1',\n",
    "        'gove_rom_stp_lmb': 's1',\n",
    "        'gove_overland': 'p2',\n",
    "        'gove_p_stp_lmb': 's2',\n",
    "        'gove_shiploader': 'p3'\n",
    "    }\n",
    "}\n",
    "\n",
    "site_alias_inv = {i: {v: k for k, v in j.items()} for i, j in site_alias.items()}\n",
    "\n",
    "\n",
    "def max_attr_value(group):\n",
    "    \"\"\"\n",
    "    Determine the appropriate attribute type for calculating maximum value based on group type.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    group : pandas.DataFrame or pandas.GroupBy\n",
    "        A DataFrame or grouped data containing an 'attr_name' column\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        'unconstrained' if the group's 'attr_name' is not 'stp_size',\n",
    "        'stp_size' otherwise\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    This helper function is used when creating histograms to determine\n",
    "    the appropriate attribute type to query for maximum values. For normal\n",
    "    production nodes, we use 'unconstrained' values, while for stockpile\n",
    "    nodes ('stp_size'), we use their own max values.\n",
    "    \"\"\"\n",
    "    return 'unconstrained' if group['attr_name'].iloc[0] != 'stp_size' else 'stp_size'\n",
    "\n",
    "\n",
    "def classify_seasons(x):\n",
    "    \"\"\"\n",
    "    Categorizes timestamps into wet and dry seasons based on month.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : pandas.DataFrame\n",
    "        DataFrame containing a 'timestamp' column with datetime values\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of strings with season labels:\n",
    "        - 'Wet season': Months from October to April (inclusive)\n",
    "        - 'Dry season': Months from May to September (inclusive)\n",
    "        - None: Any other case (should not occur with valid data)\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    Northern Australia experiences two distinct seasons:\n",
    "    - Wet season (October to April): Higher rainfall and humidity\n",
    "    - Dry season (May to September): Lower rainfall and humidity\n",
    "    \"\"\"\n",
    "    period = np.where(\n",
    "        (x.timestamp.dt.month >= 10) | (x.timestamp.dt.month <= 4), 'Wet season',\n",
    "        np.where(\n",
    "            (x.timestamp.dt.month >= 5) & (x.timestamp.dt.month <= 9), 'Dry season',\n",
    "            None\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return period\n",
    "\n",
    "\n",
    "def round_floats(df, decimals=2):\n",
    "    \"\"\"\n",
    "    Rounds all float-type columns in a Pandas DataFrame to a specified number of decimal places.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing numeric columns.\n",
    "        decimals (int, optional): The number of decimal places to round to. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with all float-type columns rounded to the specified decimal places.\n",
    "    \"\"\"\n",
    "\n",
    "    df[df.select_dtypes(include=['float']).columns] = df.select_dtypes(include=['float']).round(decimals)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_blocked_starved_days(df):\n",
    "    \"\"\"\n",
    "    Calculates the number of days a node was fully or partially blocked/starved\n",
    "    based on production constraints and stockpile levels.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        A dataframe containing production and stockpile size data for different nodes.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A dataframe with the following columns:\n",
    "        - \"node_id\": Name of the node.\n",
    "        - \"days_fully_blocked\": Days when production was fully blocked.\n",
    "        - \"days_partially_blocked\": Days when production was partially blocked.\n",
    "        - \"days_fully_starved\": Days when production was fully starved.\n",
    "        - \"days_partially_starved\": Days when production was partially starved.\n",
    "        - \"total_num_days\": Total number of days in the dataset.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - A node is **fully blocked** when its constrained production is ≤ 1 and its\n",
    "      associated stockpile is at maximum capacity.\n",
    "    - A node is **partially blocked** when constrained production is reduced\n",
    "      but still > 1, while the stockpile remains full.\n",
    "    - A node is **fully starved** when its constrained production is ≤ 1 and its\n",
    "      associated stockpile is empty.\n",
    "    - A node is **partially starved** when its constrained production is reduced\n",
    "      but still > 1, while the stockpile is empty.\n",
    "    - The function evaluates three nodes: 'amrun_loadandhaul', 'amrun_bene', and\n",
    "      'amrun_shiploader'.\n",
    "    \"\"\"\n",
    "    col = []\n",
    "    row = {}\n",
    "\n",
    "    for period in ['Wet season', 'Dry season', 'Whole year']:\n",
    "\n",
    "        row['node_id'] = 'amrun_loadandhaul'\n",
    "        row['period'] = period\n",
    "        row['days_fully_blocked'] = (\n",
    "            df\n",
    "            .query(\"period == @period\" if period != 'Whole year' else \"period == period\")\n",
    "            .query(\n",
    "            \"\"\"\n",
    "            constrained_amrun_loadandhaul <= 1 and\\\n",
    "            stp_size_amrun_rom_stp == stp_size_amrun_rom_stp.max()\n",
    "            \"\"\"\n",
    "            ).shape[0]\n",
    "        )\n",
    "        row['days_partially_blocked'] = (\n",
    "            df\n",
    "            .query(\"period == @period\" if period != 'Whole year' else \"period == period\")\n",
    "            .query(\n",
    "            \"\"\"\n",
    "            unconstrained_amrun_loadandhaul != constrained_amrun_loadandhaul and\\\n",
    "            constrained_amrun_loadandhaul > 1 and\\\n",
    "            stp_size_amrun_rom_stp == stp_size_amrun_rom_stp.max()\n",
    "            \"\"\"\n",
    "            ).shape[0]\n",
    "        )\n",
    "        row['days_fully_starved'] = 0\n",
    "        row['days_partially_starved'] = 0\n",
    "        row['total_num_days'] = df.query(\"period == @period\" if period != 'Whole year' else \"period == period\").shape[0]\n",
    "        col.append(deepcopy(row))\n",
    "\n",
    "        row['node_id'] = 'amrun_bene'\n",
    "        row['period'] = period\n",
    "        row['days_fully_blocked'] = (\n",
    "            df\n",
    "            .query(\"period == @period\" if period != 'Whole year' else \"period == period\")\n",
    "            .query(\n",
    "            \"\"\"\n",
    "            constrained_amrun_bene <= 1 and\\\n",
    "            stp_size_amrun_p_stp == stp_size_amrun_p_stp.max()\n",
    "            \"\"\"\n",
    "            ).shape[0]\n",
    "        )\n",
    "        row['days_partially_blocked'] = (\n",
    "            df\n",
    "            .query(\"period == @period\" if period != 'Whole year' else \"period == period\")\n",
    "            .query(\n",
    "            \"\"\"\n",
    "            unconstrained_amrun_bene != constrained_amrun_bene and\\\n",
    "            constrained_amrun_bene > 1 and\\\n",
    "            stp_size_amrun_p_stp == stp_size_amrun_p_stp.max()\n",
    "            \"\"\"\n",
    "            ).shape[0]\n",
    "        )\n",
    "        row['days_fully_starved'] = (\n",
    "            df\n",
    "            .query(\"period == @period\" if period != 'Whole year' else \"period == period\")\n",
    "            .query(\n",
    "            \"\"\"\n",
    "            constrained_amrun_bene <= 1 and\\\n",
    "            stp_size_amrun_rom_stp == 0\n",
    "            \"\"\"\n",
    "            ).shape[0]\n",
    "        )\n",
    "        row['days_partially_starved'] = (\n",
    "            df\n",
    "            .query(\"period == @period\" if period != 'Whole year' else \"period == period\")\n",
    "            .query(\n",
    "            \"\"\"\n",
    "            unconstrained_amrun_bene != constrained_amrun_bene and\\\n",
    "            constrained_amrun_bene > 1 and\\\n",
    "            stp_size_amrun_rom_stp == 0\n",
    "            \"\"\"\n",
    "            ).shape[0]\n",
    "        )\n",
    "        row['total_num_days'] = df.query(\"period == @period\" if period != 'Whole year' else \"period == period\").shape[0]\n",
    "        col.append(deepcopy(row))\n",
    "\n",
    "        row['node_id'] = 'amrun_shiploader'\n",
    "        row['period'] = period\n",
    "        row['days_fully_blocked'] = 0\n",
    "        row['days_partially_blocked'] = 0\n",
    "        row['days_fully_starved'] = (\n",
    "            df\n",
    "            .query(\"period == @period\" if period != 'Whole year' else \"period == period\")\n",
    "            .query(\n",
    "            \"\"\"\n",
    "            constrained_amrun_shiploader <= 1 and\\\n",
    "            stp_size_amrun_p_stp == 0\n",
    "            \"\"\"\n",
    "            ).shape[0]\n",
    "        )\n",
    "        row['days_partially_starved'] = (\n",
    "            df\n",
    "            .query(\"period == @period\" if period != 'Whole year' else \"period == period\")\n",
    "            .query(\n",
    "            \"\"\"\n",
    "            unconstrained_amrun_shiploader != constrained_amrun_shiploader and\\\n",
    "            constrained_amrun_shiploader > 1 and\\\n",
    "            stp_size_amrun_p_stp == 0\n",
    "            \"\"\"\n",
    "            ).shape[0]\n",
    "        )\n",
    "        row['total_num_days'] = df.query(\"period == @period\" if period != 'Whole year' else \"period == period\").shape[0]\n",
    "        col.append(deepcopy(row))\n",
    "\n",
    "    return pd.DataFrame(col)\n",
    "\n",
    "\n",
    "def post_process_sim_result(sim_result, attrs_dict, use_case_id='southernops_vc'):\n",
    "    \"\"\"\n",
    "    Processes simulation results to generate a structured DataFrame with calculated metrics.\n",
    "\n",
    "    This function extracts data from `sim_result`, organizes it into a Pandas DataFrame, and\n",
    "    computes various statistical summaries, including annual means, impact analysis, and\n",
    "    blocked/starved days for different nodes.\n",
    "\n",
    "    Parameters:\n",
    "        sim_result (dict): A dictionary containing simulation output data for multiple nodes.\n",
    "        use_case_id (str, optional): An identifier used to label the resulting impact DataFrame.\n",
    "                                     Defaults to 'southernops_vc'.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: A DataFrame containing the raw simulation results structured with\n",
    "                            attributes, timestamps, and node identifiers.\n",
    "            - pd.DataFrame: A DataFrame summarizing impacts, including mean values, confidence\n",
    "                            intervals, and annual metrics.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for node_name in sim_result.keys():\n",
    "        if 'stp' in node_name:\n",
    "            df = pd.concat([\n",
    "                df,\n",
    "                pd.DataFrame(\n",
    "                    sim_result[node_name]['sp_size'], columns=['attr_value']\n",
    "                ).assign(\n",
    "                    node_id=node_name,\n",
    "                    attr_name='stp_size',\n",
    "                    timestamp=pd.date_range('2022-01-01', periods=50000)\n",
    "                ).reset_index()\n",
    "            ])\n",
    "        else:\n",
    "            for attr_name in ['unconstrained', 'constrained', 'ratio']:\n",
    "                df = pd.concat([\n",
    "                    df,\n",
    "                    pd.DataFrame(\n",
    "                        sim_result[node_name]['production'][attr_name] if attr_name == 'ratio' else (\n",
    "                            sim_result[node_name]['production'][attr_name] *\n",
    "                            sim_result[node_name]['production']['ratio']\n",
    "                        ), columns=['attr_value']\n",
    "                    ).assign(\n",
    "                        node_id=node_name,\n",
    "                        attr_name=attr_name,\n",
    "                        timestamp=pd.date_range('2022-01-01', periods=50000)\n",
    "                    ).reset_index()\n",
    "                ])\n",
    "\n",
    "    p_stp = (\n",
    "        df\n",
    "        .assign(period=lambda x: classify_seasons(x))\n",
    "        .query(\" node_id == 'amrun_p_stp' & attr_name == 'stp_size' \")\n",
    "    )\n",
    "\n",
    "    shiploader = (\n",
    "        df\n",
    "        .assign(period=lambda x: classify_seasons(x))\n",
    "        .query(\" node_id == 'amrun_shiploader' & attr_name == 'constrained' \")\n",
    "    )\n",
    "\n",
    "    bene = (\n",
    "        df\n",
    "        .assign(period=lambda x: classify_seasons(x))\n",
    "        .query(\" node_id == 'amrun_bene' & attr_name == 'constrained' \")\n",
    "    )\n",
    "\n",
    "    inventory_summary = pd.DataFrame(\n",
    "        {\n",
    "            'node_id': ['amrun_p_stp', 'amrun_p_stp', 'amrun_p_stp'],\n",
    "            'value_chain_node_id': [\n",
    "                'southernops_vc_amrun_p_stp', 'southernops_vc_amrun_p_stp', 'southernops_vc_amrun_p_stp'\n",
    "            ],\n",
    "            'period': ['Dry season', 'Wet season', 'Whole year'],\n",
    "            'mean_days_to_empty': [\n",
    "                divide(\n",
    "                    p_stp.query(\"period == 'Dry season'\").attr_value.mean(),\n",
    "                    shiploader.query(\"period == 'Dry season'\").attr_value.mean()\n",
    "                ),\n",
    "                divide(\n",
    "                    p_stp.query(\"period == 'Wet season'\").attr_value.mean(),\n",
    "                    shiploader.query(\"period == 'Wet season'\").attr_value.mean()\n",
    "                ),\n",
    "                divide(\n",
    "                    p_stp.attr_value.mean(),\n",
    "                    shiploader.attr_value.mean()\n",
    "                )\n",
    "            ],\n",
    "            'mean_days_to_full': [\n",
    "                divide(\n",
    "                    (\n",
    "                        attrs_dict['sp_attr']['amrun_p_stp']['live_size'] -\n",
    "                        p_stp.query(\"period == 'Dry season'\").attr_value.mean()\n",
    "                    ),\n",
    "                    bene.query(\"period == 'Dry season'\").attr_value.mean()\n",
    "                ),\n",
    "                divide(\n",
    "                    (\n",
    "                        attrs_dict['sp_attr']['amrun_p_stp']['live_size'] -\n",
    "                        p_stp.query(\"period == 'Wet season'\").attr_value.mean()\n",
    "                    ),\n",
    "                    bene.query(\"period == 'Wet season'\").attr_value.mean()\n",
    "                ),\n",
    "                divide(\n",
    "                    (attrs_dict['sp_attr']['amrun_p_stp']['live_size'] - p_stp.attr_value.mean()),\n",
    "                    bene.attr_value.mean()\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    full_year_annual_means = (\n",
    "        df\n",
    "        .assign(\n",
    "            period=\"Whole year\",\n",
    "            year_index=lambda x: np.ceil((x['index'] + 1) / 365)\n",
    "        ).query(\"~attr_name.isin(['stp_size', 'ratio'])\")\n",
    "        .groupby(\n",
    "            ['node_id', 'attr_name', 'period', 'year_index'],\n",
    "            as_index=False,\n",
    "            group_keys=False\n",
    "        ).agg(\n",
    "            mean=pd.NamedAgg(column='attr_value', aggfunc='sum'),\n",
    "            num_days=pd.NamedAgg(column='attr_value', aggfunc='count'),\n",
    "        ).query(\" num_days == 365 \")\n",
    "        .groupby(['node_id', 'attr_name', 'period'], as_index=False)\n",
    "        .agg(\n",
    "            annual_mean=pd.NamedAgg(column='mean', aggfunc='mean'),\n",
    "            annual_mean_upper=pd.NamedAgg(column='mean', aggfunc=lambda x: np.mean(x) + 2 * np.std(x)),\n",
    "            annual_mean_lower=pd.NamedAgg(column='mean', aggfunc=lambda x: np.mean(x) - 2 * np.std(x))\n",
    "        ).pivot(\n",
    "            index=['node_id', 'period'],\n",
    "            columns='attr_name',\n",
    "            values=['annual_mean', 'annual_mean_upper', 'annual_mean_lower']\n",
    "        ).reset_index()\n",
    "        .pipe(\n",
    "            lambda d: d.set_axis(\n",
    "                [\n",
    "                    f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) and '' not in col else col[0]\n",
    "                    for col in d.columns\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    seasonal_annual_means = (\n",
    "        df\n",
    "        .assign(\n",
    "            period=lambda x: classify_seasons(x),\n",
    "            year_index=lambda x: np.ceil((x['index'] + 1) / 365)\n",
    "        ).query(\"~attr_name.isin(['stp_size', 'ratio'])\")\n",
    "        .groupby(\n",
    "            ['node_id', 'attr_name', 'period', 'year_index'],\n",
    "            as_index=False,\n",
    "            group_keys=False\n",
    "        ).agg(\n",
    "            mean=pd.NamedAgg(column='attr_value', aggfunc='sum'),\n",
    "            num_days=pd.NamedAgg(column='attr_value', aggfunc='count'),\n",
    "        ).assign(\n",
    "            median_num_days=lambda x: x.groupby(['period'])['num_days'].transform('median')\n",
    "        ).query(\" num_days >= median_num_days*0.95 \")\n",
    "        .groupby(['node_id', 'attr_name', 'period'], as_index=False)\n",
    "        .agg(\n",
    "            annual_mean=pd.NamedAgg(column='mean', aggfunc='mean'),\n",
    "            annual_mean_upper=pd.NamedAgg(column='mean', aggfunc=lambda x: np.mean(x) + 2 * np.std(x)),\n",
    "            annual_mean_lower=pd.NamedAgg(column='mean', aggfunc=lambda x: np.mean(x) - 2 * np.std(x))\n",
    "        ).pivot(\n",
    "            index=['node_id', 'period'],\n",
    "            columns='attr_name',\n",
    "            values=['annual_mean', 'annual_mean_upper', 'annual_mean_lower']\n",
    "        ).reset_index()\n",
    "        .pipe(\n",
    "            lambda d: d.set_axis(\n",
    "                [\n",
    "                    f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) and '' not in col else col[0]\n",
    "                    for col in d.columns\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    annual_means = pd.concat([full_year_annual_means, seasonal_annual_means], ignore_index=True)\n",
    "\n",
    "    starved_blocked_days = (\n",
    "        df.assign(\n",
    "            period=lambda x: classify_seasons(x)\n",
    "        ).pivot(\n",
    "            index=['index', 'period'],\n",
    "            columns=['attr_name', 'node_id'],\n",
    "            values=['attr_value']\n",
    "        ).reset_index()\n",
    "        .pipe(\n",
    "            lambda d: d.set_axis(\n",
    "                [\n",
    "                    f\"{col[1]}_{col[2]}\" if isinstance(col, tuple) and '' not in col else col[0]\n",
    "                    for col in d.columns\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "        ).assign(\n",
    "            stp_size_amrun_rom_stp=lambda x: x.stp_size_amrun_rom_stp.shift(1).fillna(0),\n",
    "            stp_size_amrun_p_stp=lambda x: x.stp_size_amrun_p_stp.shift(1).fillna(0)\n",
    "        ).pipe(\n",
    "            calc_blocked_starved_days\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fullyear_summary = (\n",
    "        df\n",
    "        .assign(\n",
    "            period=\"Whole year\"\n",
    "        ).groupby(\n",
    "            ['node_id', 'attr_name', 'period'],\n",
    "            as_index=False\n",
    "        ).agg(\n",
    "            mean=pd.NamedAgg(column='attr_value', aggfunc='mean'),\n",
    "            std=pd.NamedAgg(column='attr_value', aggfunc='std'),\n",
    "            first_quartile=pd.NamedAgg(column='attr_value', aggfunc=lambda x: np.percentile(x, 25)),\n",
    "            median=pd.NamedAgg(column='attr_value', aggfunc='median'),\n",
    "            third_quartile=pd.NamedAgg(column='attr_value', aggfunc=lambda x: np.percentile(x, 75)),\n",
    "            max=pd.NamedAgg(column='attr_value', aggfunc='max'),\n",
    "            sem=pd.NamedAgg(column='attr_value', aggfunc=stats.sem)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    seasonal_summary = (\n",
    "        df\n",
    "        .assign(\n",
    "            period=lambda x: classify_seasons(x)\n",
    "        ).groupby(\n",
    "            ['node_id', 'attr_name', 'period'],\n",
    "            as_index=False\n",
    "        ).agg(\n",
    "            mean=pd.NamedAgg(column='attr_value', aggfunc='mean'),\n",
    "            std=pd.NamedAgg(column='attr_value', aggfunc='std'),\n",
    "            first_quartile=pd.NamedAgg(column='attr_value', aggfunc=lambda x: np.percentile(x, 25)),\n",
    "            median=pd.NamedAgg(column='attr_value', aggfunc='median'),\n",
    "            third_quartile=pd.NamedAgg(column='attr_value', aggfunc=lambda x: np.percentile(x, 75)),\n",
    "            max=pd.NamedAgg(column='attr_value', aggfunc='max'),\n",
    "            sem=pd.NamedAgg(column='attr_value', aggfunc=stats.sem)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    seasonal_bins = (\n",
    "        df\n",
    "        .query(\"attr_name in ['constrained', 'unconstrained', 'stp_size']\")\n",
    "        .assign(period=lambda x: classify_seasons(x))\n",
    "        .groupby(['node_id', 'period', 'attr_name'], as_index=False)\n",
    "        [['node_id', 'period', 'attr_name', 'attr_value']]\n",
    "        .apply(\n",
    "            lambda group: pd.DataFrame({\n",
    "                'node_id': group['node_id'].iloc[0],\n",
    "                'period': group['period'].iloc[0],\n",
    "                'type': group['attr_name'].iloc[0],\n",
    "\n",
    "                'count': np.histogram(\n",
    "                    group['attr_value'],\n",
    "                    bins=np.arange(\n",
    "                        0, df.query(f\"attr_name == '{max_attr_value(group)}'\").attr_value.max(),\n",
    "                        20000 if group['attr_name'].iloc[0] == 'stp_size' else 10000\n",
    "                    )\n",
    "                )[0],\n",
    "                'bins_start': np.histogram(\n",
    "                    group['attr_value'],\n",
    "                    bins=np.arange(\n",
    "                        0, df.query(f\"attr_name == '{max_attr_value(group)}'\").attr_value.max(),\n",
    "                        20000 if group['attr_name'].iloc[0] == 'stp_size' else 10000\n",
    "                    )\n",
    "                )[1][:-1],\n",
    "                'bins_end': np.histogram(\n",
    "                    group['attr_value'],\n",
    "                    bins=np.arange(\n",
    "                        0, df.query(f\"attr_name == '{max_attr_value(group)}'\").attr_value.max(),\n",
    "                        20000 if group['attr_name'].iloc[0] == 'stp_size' else 10000\n",
    "                    )\n",
    "                )[1][1:]\n",
    "            })\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fullyear_bins = (\n",
    "        df\n",
    "        .query(\"attr_name in ['constrained', 'unconstrained', 'stp_size']\")\n",
    "        .assign(period=\"Whole year\")\n",
    "        .groupby(['node_id', 'period', 'attr_name'], as_index=False)\n",
    "        [['node_id', 'period', 'attr_name', 'attr_value']]\n",
    "        .apply(\n",
    "            lambda group: pd.DataFrame({\n",
    "                'node_id': group['node_id'].iloc[0],\n",
    "                'period': group['period'].iloc[0],\n",
    "                'type': group['attr_name'].iloc[0],\n",
    "\n",
    "                'count': np.histogram(\n",
    "                    group['attr_value'],\n",
    "                    bins=np.arange(\n",
    "                        0, df.query(f\"attr_name == '{max_attr_value(group)}'\").attr_value.max(),\n",
    "                        20000 if group['attr_name'].iloc[0] == 'stp_size' else 10000\n",
    "                    )\n",
    "                )[0],\n",
    "                'bins_start': np.histogram(\n",
    "                    group['attr_value'],\n",
    "                    bins=np.arange(\n",
    "                        0, df.query(f\"attr_name == '{max_attr_value(group)}'\").attr_value.max(),\n",
    "                        20000 if group['attr_name'].iloc[0] == 'stp_size' else 10000\n",
    "                    )\n",
    "                )[1][:-1],\n",
    "                'bins_end': np.histogram(\n",
    "                    group['attr_value'],\n",
    "                    bins=np.arange(\n",
    "                        0, df.query(f\"attr_name == '{max_attr_value(group)}'\").attr_value.max(),\n",
    "                        20000 if group['attr_name'].iloc[0] == 'stp_size' else 10000\n",
    "                    )\n",
    "                )[1][1:]\n",
    "            })\n",
    "        )\n",
    "    )\n",
    "\n",
    "    bins_all = (\n",
    "        pd.concat([seasonal_bins, fullyear_bins], ignore_index=True)\n",
    "        .groupby(\n",
    "            ['node_id', 'period', 'type'],\n",
    "            as_index=False,\n",
    "            group_keys=False\n",
    "        )[['node_id', 'period', 'type', 'count', 'bins_start', 'bins_end']]\n",
    "        .apply(\n",
    "            lambda group: (\n",
    "                group\n",
    "                .sort_values('bins_start', ascending=True)\n",
    "                .query(\"~(count == 0 & count.shift() == 0)\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    impact = (\n",
    "        pd.concat(\n",
    "            [seasonal_summary, fullyear_summary],\n",
    "            ignore_index=True\n",
    "        ).query(\" attr_name != 'stp_size' \")\n",
    "        .pivot(\n",
    "            index=['node_id', 'period'],\n",
    "            columns='attr_name',\n",
    "            values=['mean', 'first_quartile', 'median', 'third_quartile', 'max', 'std', 'sem']\n",
    "        ).reset_index()\n",
    "        .pipe(\n",
    "            lambda d: d.set_axis(\n",
    "                [\n",
    "                    f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) and '' not in col else col[0]\n",
    "                    for col in d.columns\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "        ).assign(\n",
    "            value_chain_node_id=lambda x: use_case_id + '_' + x.node_id,\n",
    "\n",
    "            impact_tons=lambda x: x.mean_constrained - x.mean_unconstrained,\n",
    "\n",
    "            impact_tons_upper=lambda x: (\n",
    "                (x.mean_constrained - x.mean_unconstrained)\n",
    "                + 2 * np.sqrt(x.sem_constrained**2 + x.sem_unconstrained**2)\n",
    "            ),\n",
    "\n",
    "            impact_tons_lower=lambda x: (\n",
    "                (x.mean_constrained - x.mean_unconstrained)\n",
    "                - 2 * np.sqrt(x.sem_constrained**2 + x.sem_unconstrained**2)\n",
    "            ),\n",
    "\n",
    "            impact=lambda x: (x.mean_constrained - x.mean_unconstrained) / x.mean_unconstrained,\n",
    "\n",
    "            impact_upper=lambda x: (\n",
    "                (x.mean_constrained - x.mean_unconstrained)\n",
    "                + 2 * np.sqrt(x.sem_constrained**2 + x.sem_unconstrained**2)\n",
    "            ) / x.mean_unconstrained,\n",
    "\n",
    "            impact_lower=lambda x: (\n",
    "                (x.mean_constrained - x.mean_unconstrained)\n",
    "                - 2 * np.sqrt(x.sem_constrained**2 + x.sem_unconstrained**2)\n",
    "            ) / x.mean_unconstrained\n",
    "        ).merge(\n",
    "            annual_means, on=['node_id', 'period'], how='left'\n",
    "        ).merge(\n",
    "            starved_blocked_days, on=['node_id', 'period'], how='left'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    impact = pd.concat([impact, inventory_summary])\n",
    "\n",
    "    return df, impact, bins_all\n",
    "\n",
    "\n",
    "def calc_constrained_prod(**kwargs):\n",
    "    \"\"\"\n",
    "    Solves a linear optimization problem to determine constrained production levels.\n",
    "\n",
    "    This function uses Google's OR-Tools linear solver ('GLOP') to maximize production\n",
    "    while adhering to given constraints on resources and capacities. It is specifically\n",
    "    designed for use cases 'southernops_vc' and 'gove_vc'.\n",
    "\n",
    "    Parameters:\n",
    "    **kwargs: Arbitrary keyword arguments containing the following keys:\n",
    "        - use_case_id (str): The identifier for the use case.\n",
    "        - p1_max, p2_max, p3_max (float): Maximum allowable production levels for different\n",
    "          production stages.\n",
    "        - r1, r2 (float): Conversion ratios between production stages.\n",
    "        - s1, s2 (float): Initial stock levels for different stages.\n",
    "        - s1_max, s2_max (float): Maximum allowable stock levels.\n",
    "\n",
    "    Returns:\n",
    "    dict or bool:\n",
    "        - A dictionary with production variables as keys, each containing:\n",
    "            - 'value': The computed optimal value of the variable.\n",
    "            - 'is_constrained': Boolean indicating whether the variable is at its upper limit.\n",
    "        - False if the use case is not supported.\n",
    "    \"\"\"\n",
    "    solver = pywraplp.Solver.CreateSolver('GLOP')\n",
    "\n",
    "    if kwargs['use_case_id'] in ['southernops_vc', 'gove_vc']:\n",
    "\n",
    "        p1 = solver.NumVar(lb=0, ub=kwargs['p1_max'], name='p1')\n",
    "        p11 = solver.NumVar(lb=0, ub=kwargs['p1_max'], name='p11')\n",
    "        p12 = solver.NumVar(lb=0, ub=kwargs['p1_max'], name='p12')\n",
    "        p13 = solver.NumVar(lb=0, ub=kwargs['p2_max'], name='p13')\n",
    "        p2 = solver.NumVar(lb=0, ub=kwargs['p2_max'], name='p2')\n",
    "        p3 = solver.NumVar(lb=0, ub=kwargs['p3_max'], name='p3')\n",
    "\n",
    "        solver.Maximize(p1 + p2 + p3)\n",
    "\n",
    "        solver.Add(p11 + p12 == p1 * kwargs['r1'])\n",
    "        solver.Add(p12 + p13 == p2)\n",
    "        solver.Add(p11 + kwargs['s1'] - p13 <= kwargs['s1_max'])\n",
    "        solver.Add(p11 + kwargs['s1'] - p13 >= 0)\n",
    "        solver.Add(p2 * kwargs['r2'] + kwargs['s2'] - p3 <= kwargs['s2_max'])\n",
    "        solver.Add(p2 * kwargs['r2'] + kwargs['s2'] - p3 >= 0)\n",
    "\n",
    "        solver.Solve()\n",
    "\n",
    "        return_dict = {\n",
    "            var.name(): {'value': var.solution_value()} for var in solver.variables()\n",
    "        }\n",
    "\n",
    "        return_dict['s1'] = return_dict['p11']['value'] + kwargs['s1'] - return_dict['p13']['value']\n",
    "        return_dict['s2'] = return_dict['p2']['value'] * kwargs['r2'] + kwargs['s2'] - return_dict['p3']['value']\n",
    "\n",
    "        keys_to_delete = ['p11', 'p12', 'p13']\n",
    "        for del_key in keys_to_delete:\n",
    "            del return_dict[del_key]\n",
    "\n",
    "        for ret_key in return_dict.keys():\n",
    "            if ret_key.startswith('p'):\n",
    "                return_dict[ret_key]['is_constrained'] = not isclose(\n",
    "                    kwargs[ret_key + '_max'],\n",
    "                    return_dict[ret_key]['value']\n",
    "                )\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def calc_constrained_prod_norom(**kwargs):\n",
    "    \"\"\"\n",
    "    Solves a linear optimization problem to determine constrained production levels.\n",
    "\n",
    "    This function uses Google's OR-Tools linear solver ('GLOP') to maximize production\n",
    "    while adhering to given constraints on resources and capacities. It is specifically\n",
    "    designed for use cases 'southernops_vc' and 'gove_vc'.\n",
    "\n",
    "    Parameters:\n",
    "    **kwargs: Arbitrary keyword arguments containing the following keys:\n",
    "        - use_case_id (str): The identifier for the use case.\n",
    "        - p1_max, p2_max, p3_max (float): Maximum allowable production levels for different\n",
    "          production stages.\n",
    "        - r1, r2 (float): Conversion ratios between production stages.\n",
    "        - s1, s2 (float): Initial stock levels for different stages.\n",
    "        - s1_max, s2_max (float): Maximum allowable stock levels.\n",
    "\n",
    "    Returns:\n",
    "    dict or bool:\n",
    "        - A dictionary with production variables as keys, each containing:\n",
    "            - 'value': The computed optimal value of the variable.\n",
    "            - 'is_constrained': Boolean indicating whether the variable is at its upper limit.\n",
    "        - False if the use case is not supported.\n",
    "    \"\"\"\n",
    "    solver = pywraplp.Solver.CreateSolver('GLOP')\n",
    "\n",
    "    p1 = solver.NumVar(lb=0, ub=kwargs['p1_max'], name='p1')\n",
    "    p2 = solver.NumVar(lb=0, ub=kwargs['p2_max'], name='p2')\n",
    "    p3 = solver.NumVar(lb=0, ub=kwargs['p3_max'], name='p3')\n",
    "\n",
    "    solver.Maximize(p1 + p2 + p3)\n",
    "\n",
    "    solver.Add(p1 * kwargs['r1'] == p2)\n",
    "    solver.Add(p2 * kwargs['r2'] + kwargs['s2'] - p3 <= kwargs['s2_max'])\n",
    "    solver.Add(p2 * kwargs['r2'] + kwargs['s2'] - p3 >= 0)\n",
    "\n",
    "    solver.Solve()\n",
    "\n",
    "    return_dict = {\n",
    "        var.name(): {'value': var.solution_value()} for var in solver.variables()\n",
    "    }\n",
    "\n",
    "    return_dict['s2'] = return_dict['p2']['value'] * kwargs['r2'] + kwargs['s2'] - return_dict['p3']['value']\n",
    "\n",
    "    for ret_key in return_dict.keys():\n",
    "        if ret_key.startswith('p'):\n",
    "            return_dict[ret_key]['is_constrained'] = not isclose(\n",
    "                kwargs[ret_key + '_max'],\n",
    "                return_dict[ret_key]['value']\n",
    "            )\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "\n",
    "def calc_attributes(\n",
    "    num_iters: int,\n",
    "    nodes: pd.DataFrame,\n",
    "    gt_value_chain_timeseries: pd.DataFrame\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Constructs a dictionary of attributes for nodes and stockpiles in a value chain simulation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    num_iters : int\n",
    "        The number of iterations for which attributes will be generated.\n",
    "    nodes : pd.DataFrame\n",
    "        A dataframe containing metadata about nodes, including `node_id` and `node_type`.\n",
    "    gt_value_chain_timeseries : pd.DataFrame\n",
    "        A dataframe containing time-series data for nodes, including production metrics\n",
    "        (`input_tons`, `output_tons`), capacity (`max_capacity`, `min_capacity`), and inventory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary with the following structure:\n",
    "        - \"size\": Number of iterations.\n",
    "        - \"num_nodes\": Total number of transformation nodes.\n",
    "        - \"node_names\": List of transformation node IDs.\n",
    "        - \"sp_names\": List of stockpile node IDs.\n",
    "        - \"node_attr\": Dictionary mapping transformation nodes to their attributes:\n",
    "            - \"smoothed_trend\": Median of recent trend component from seasonal decomposition.\n",
    "            - \"residual_mean\": Mean of residual component from decomposition.\n",
    "            - \"residual_std\": Standard deviation of residuals.\n",
    "            - \"seasonal_pattern\": Seasonal component as a NumPy array.\n",
    "            - \"ratio_mean\": Mean efficiency ratio (output/input).\n",
    "            - \"ratio_std\": Standard deviation of efficiency ratio.\n",
    "            - \"generated_downtime\": Simulated downtime probabilities using an autoregressive model.\n",
    "            - \"size\": Number of iterations.\n",
    "        - \"sp_attr\": Dictionary mapping stockpile nodes to their attributes:\n",
    "            - \"live_size\": Median full capacity (`max_capacity - min_capacity`).\n",
    "            - \"sp_size\": Initial stockpile size (set to zero).\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function models downtime using an autoregressive time series model.\n",
    "    - Seasonal decomposition is applied to `input_tons` to extract trend, seasonality, and residuals.\n",
    "    - The function ensures that output does not exceed input for transformation nodes.\n",
    "    \"\"\"\n",
    "    attrs_dict = {\n",
    "        \"size\" : num_iters,\n",
    "        \"num_nodes\" : None,\n",
    "        \"node_names\" : [],\n",
    "        \"sp_names\" : [],\n",
    "        \"node_attr\": {},\n",
    "        \"sp_attr\": {}\n",
    "    }\n",
    "\n",
    "    for idx, row in nodes.iterrows():\n",
    "        if row['node_type'] == 'Transformation':\n",
    "            input_output = (\n",
    "                gt_value_chain_timeseries\n",
    "                .query(f\" node_id == '{row['node_id']}' and metric in ['input_tons', 'output_tons'] \")\n",
    "                .pivot(index='timestamp', columns='metric', values='value')\n",
    "                .reset_index()\n",
    "                .query(\"output_tons <= input_tons\")\n",
    "                .assign(\n",
    "                    output_tons=lambda x: np.where(\n",
    "                        (x.output_tons == 0) & (x.input_tons > 0), x.input_tons, x.output_tons\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # input_tons = (\n",
    "            #     input_output\n",
    "            #     .replace(0, np.nan)\n",
    "            #     .assign(\n",
    "            #         input_tons=lambda x: x.input_tons.interpolate(limit_direction='both'),\n",
    "            #         output_tons=lambda x: x.output_tons.interpolate(limit_direction='both')\n",
    "            #     ).filter(['timestamp', 'input_tons'])\n",
    "            #     .set_index('timestamp')\n",
    "            # )\n",
    "\n",
    "            input_tons = (\n",
    "                input_output\n",
    "                .query(\" input_tons > 0 \")\n",
    "                .set_index('timestamp')\n",
    "            )\n",
    "\n",
    "            result = seasonal_decompose(input_tons['input_tons'], model='additive', period=365, extrapolate_trend=90)\n",
    "            trend = result.trend\n",
    "            seasonal = result.seasonal\n",
    "            residuals = result.resid\n",
    "\n",
    "            prev_year = datetime.today().year - 1\n",
    "\n",
    "            mean_trend_mean_ratio = (\n",
    "                input_tons\n",
    "                .assign(\n",
    "                    moving_avg_360d=lambda x: x['input_tons'].rolling(window=360, min_periods=1).mean(),\n",
    "                    trend=trend\n",
    "                ).query(f'timestamp.dt.year >= {prev_year}')\n",
    "                .assign(trend_mean_ratio=lambda x: x.trend / x.moving_avg_360d)\n",
    "                .trend_mean_ratio\n",
    "                .mean()\n",
    "            )\n",
    "\n",
    "            mean_max_trend_ratio = (\n",
    "                input_tons\n",
    "                .assign(\n",
    "                    moving_max_360d=lambda x: x['input_tons'].rolling(window=360, min_periods=1).max(),\n",
    "                    moving_trend_360d=trend.rolling(window=360, min_periods=1).mean()\n",
    "                ).query(f'timestamp.dt.year >= {prev_year}')\n",
    "                .assign(max_trend_ratio=lambda x: x.moving_max_360d / x.moving_trend_360d)\n",
    "                .max_trend_ratio\n",
    "                .mean()\n",
    "            )\n",
    "\n",
    "            ratio = (\n",
    "                input_output\n",
    "                .query(\" input_tons > 0 \")\n",
    "                .assign(\n",
    "                    ratio=lambda x: x.output_tons / x.input_tons\n",
    "                )\n",
    "                .ratio\n",
    "            )\n",
    "\n",
    "            downtime_data = (\n",
    "                input_output\n",
    "                .assign(shutdown=lambda x: np.where(x.input_tons == 0, 1, 0))\n",
    "                .shutdown\n",
    "                .to_numpy()\n",
    "            )\n",
    "            downtime_model = AutoReg(downtime_data, lags=2, trend='n', seasonal=True, period=365).fit()\n",
    "            forecasted_probability = downtime_model.forecast(num_iters)\n",
    "\n",
    "            attrs_dict['node_attr'][row['node_id']] = {\n",
    "                'hist_max_prod': input_tons['input_tons'].max(),\n",
    "                'smoothed_trend': trend[-60:].median(),\n",
    "                'mean_trend_mean_ratio': mean_trend_mean_ratio,\n",
    "                'mean_max_trend_ratio': mean_max_trend_ratio,\n",
    "                'residual_mean': residuals.mean(),\n",
    "                'residual_std': residuals.std(),\n",
    "                'seasonal_pattern': seasonal[:365].to_numpy(),\n",
    "                'ratio_mean' : ratio.mean(),\n",
    "                'ratio_std': ratio.std(),\n",
    "                'generated_downtime': np.random.binomial(1, np.clip(forecasted_probability, 0, 1)),\n",
    "                'size': num_iters\n",
    "            }\n",
    "\n",
    "            attrs_dict['node_names'].append(row['node_id'])\n",
    "\n",
    "        else:\n",
    "            max_cap = (\n",
    "                gt_value_chain_timeseries\n",
    "                .query(f\" node_id == '{row['node_id']}' and metric in ['max_capacity', 'min_capacity'] \")\n",
    "                .pivot(index='timestamp', columns='metric', values='value')\n",
    "                .reset_index()\n",
    "                .assign(\n",
    "                    full_capacity=lambda x: x.max_capacity - x.min_capacity\n",
    "                )\n",
    "                .full_capacity\n",
    "                .median()\n",
    "            )\n",
    "\n",
    "            sp_size = (\n",
    "                gt_value_chain_timeseries\n",
    "                .query(f\" node_id == '{row['node_id']}' and metric == 'inventory' \")\n",
    "                .value\n",
    "                .median()\n",
    "            )\n",
    "\n",
    "            attrs_dict['sp_attr'][row['node_id']] = {\n",
    "                'live_size' : max_cap,\n",
    "                'sp_size' : sp_size\n",
    "            }\n",
    "\n",
    "            attrs_dict['sp_names'].append(row['node_id'])\n",
    "\n",
    "    attrs_dict['num_nodes'] = len(attrs_dict['node_names'])\n",
    "    return attrs_dict\n",
    "\n",
    "\n",
    "class SimCapacity:\n",
    "    \"\"\"\n",
    "    A class to simulate production capacity and constraints for various nodes in a supply chain.\n",
    "\n",
    "    This class generates unconstrained production values, applies constraints, and computes\n",
    "    simulation results based on provided attributes.\n",
    "\n",
    "    Attributes:\n",
    "    use_case_id (str): The identifier for the specific use case.\n",
    "    exclude_rom_stp (bool): Flag indicating whether ROM-STP constraints should be excluded.\n",
    "    attrs_dict (dict): A deep copy of the input attributes dictionary.\n",
    "    sim_result (dict): A dictionary containing the simulation results.\n",
    "\n",
    "    Methods:\n",
    "    gen_unconstrained_prod(size, smoothed_trend, seasonal_pattern, generated_downtime,\n",
    "                           ratio_mean, ratio_std, residual_mean, residual_std) -> np.array:\n",
    "        Generates unconstrained production values based on a trend, seasonal pattern,\n",
    "        and stochastic variations.\n",
    "\n",
    "    gen_sim_result(attrs_dict) -> dict:\n",
    "        Generates a full simulation result, computing both unconstrained and constrained\n",
    "        production values across multiple nodes.\n",
    "\n",
    "    rerun_sim_with_scenario(params) -> dict:\n",
    "        Creates a modified simulation scenario by adjusting target attributes in the\n",
    "        attributes dictionary and regenerating simulation results.\n",
    "    \"\"\"\n",
    "    def __init__(self, attrs_dict):\n",
    "        self.use_case_id = attrs_dict['use_case_id']\n",
    "        self.exclude_rom_stp = attrs_dict['exclude_rom_stp']\n",
    "        self.attrs_dict = deepcopy(attrs_dict)\n",
    "        self.sim_result = self.gen_sim_result(self.attrs_dict)\n",
    "\n",
    "    def gen_unconstrained_prod(\n",
    "        self, size: int, smoothed_trend: float, mean_trend_mean_ratio: float, mean_max_trend_ratio: float,\n",
    "        seasonal_pattern: np.array, generated_downtime: np.array, ratio_mean: float, ratio_std: float,\n",
    "        residual_mean: float, residual_std: float, hist_max_prod: float\n",
    "    ) -> np.array:\n",
    "        generated_trend = np.tile(smoothed_trend, size)\n",
    "        generated_season = np.tile(seasonal_pattern, int(size / 365) + 1)[:size]\n",
    "        generated_residual = stats.norm.rvs(residual_mean, residual_std, size)\n",
    "        unconstrained_prod = np.clip(\n",
    "            generated_trend + generated_season + generated_residual,\n",
    "            a_min=0, a_max=hist_max_prod\n",
    "        )\n",
    "\n",
    "        ratio = stats.truncnorm.rvs(\n",
    "            a=(0 - ratio_mean) / (1e-100 if ratio_std == 0 else ratio_std),\n",
    "            b=(1 - ratio_mean) / (1e-100 if ratio_std == 0 else ratio_std),\n",
    "            loc=ratio_mean,\n",
    "            scale=ratio_std,\n",
    "            size=size\n",
    "        )\n",
    "\n",
    "        return np.where(generated_downtime[:size] == 1, 0, unconstrained_prod), ratio\n",
    "\n",
    "    def gen_sim_result(self, attrs_dict):\n",
    "        sim_result = {}\n",
    "        for node_name in attrs_dict['node_names']:\n",
    "            sim_result[node_name] = {'production': {}}\n",
    "\n",
    "            unconstrained_prod, ratio = self.gen_unconstrained_prod(\n",
    "                **attrs_dict['node_attr'][node_name]\n",
    "            )\n",
    "\n",
    "            sim_result[node_name]['production']['unconstrained'] = unconstrained_prod\n",
    "            sim_result[node_name]['production']['ratio'] = ratio\n",
    "            sim_result[node_name]['production']['constrained'] = unconstrained_prod.copy()\n",
    "\n",
    "        for sp_name in attrs_dict['sp_names']:\n",
    "            sim_result[sp_name] = {\n",
    "                'current_sp': attrs_dict['sp_attr'][sp_name]['sp_size'],\n",
    "                'sp_size': np.zeros(attrs_dict['size']),\n",
    "                'live_size': attrs_dict['sp_attr'][sp_name]['live_size']\n",
    "            }\n",
    "\n",
    "        for i in range(attrs_dict['size']):\n",
    "            init_val = {}\n",
    "            for key in sim_result.keys():\n",
    "                if 'production' in sim_result[key].keys():\n",
    "                    init_val[\n",
    "                        site_alias[self.use_case_id][key] + '_max'\n",
    "                    ] = sim_result[key]['production']['unconstrained'][i]\n",
    "\n",
    "                    init_val[\n",
    "                        site_alias[self.use_case_id][key].replace('p', 'r')\n",
    "                    ] = sim_result[key]['production']['ratio'][i]\n",
    "                else:\n",
    "                    init_val[site_alias[self.use_case_id][key]] = sim_result[key]['current_sp']\n",
    "                    init_val[site_alias[self.use_case_id][key] + '_max'] = sim_result[key]['live_size']\n",
    "\n",
    "            init_val['use_case_id'] = self.use_case_id\n",
    "\n",
    "            if attrs_dict['exclude_rom_stp']:\n",
    "                constrained_prod = calc_constrained_prod_norom(**init_val)\n",
    "            else:\n",
    "                constrained_prod = calc_constrained_prod(**init_val)\n",
    "\n",
    "            for key in constrained_prod.keys():\n",
    "                if key.startswith('p'):\n",
    "                    sim_result[\n",
    "                        site_alias_inv[self.use_case_id][key]\n",
    "                    ]['production']['constrained'][i] = constrained_prod[key]['value']\n",
    "\n",
    "                else:\n",
    "                    sim_result[\n",
    "                        site_alias_inv[self.use_case_id][key]\n",
    "                    ]['current_sp'] = constrained_prod[key]\n",
    "\n",
    "                    sim_result[\n",
    "                        site_alias_inv[self.use_case_id][key]\n",
    "                    ]['sp_size'][i] = constrained_prod[key]\n",
    "\n",
    "        for sp_name in attrs_dict['sp_names']:\n",
    "            for key_to_del in ['live_size', 'current_sp']:\n",
    "                del sim_result[sp_name][key_to_del]\n",
    "\n",
    "        return sim_result\n",
    "\n",
    "    def rerun_sim_with_scenario(\n",
    "        self,\n",
    "        params=[{\n",
    "            'attr2change': 'node_attr',\n",
    "            'node_id': 'amrun_shiploader',\n",
    "            'target_var': 'smoothed_trend',\n",
    "            'is_numeric': True,\n",
    "            'taget_value': 102489.89,\n",
    "            'exclude_rom_stp': False\n",
    "        }]\n",
    "    ):\n",
    "        attrs_dict = deepcopy(self.attrs_dict)\n",
    "\n",
    "        for param in params:\n",
    "            attr2change, node_id, target_var, is_numeric, taget_value = (\n",
    "                param['attr2change'], param['node_id'], param['target_var'],\n",
    "                param['is_numeric'], param['taget_value']\n",
    "            )\n",
    "\n",
    "            attrs_dict['exclude_rom_stp'] = param['exclude_rom_stp']\n",
    "\n",
    "            if attr2change == 'node_attr' and is_numeric:\n",
    "                attrs_dict[attr2change][node_id][target_var] = taget_value\n",
    "\n",
    "            elif attr2change == 'node_attr':\n",
    "                attrs_dict[attr2change][node_id][target_var] += abs(\n",
    "                    attrs_dict[attr2change][node_id][target_var]\n",
    "                ) * taget_value\n",
    "\n",
    "            else:\n",
    "                attrs_dict[attr2change][node_id][target_var] *= (\n",
    "                    1 +\n",
    "                    taget_value\n",
    "                )\n",
    "\n",
    "        sim_result = self.gen_sim_result(attrs_dict)\n",
    "\n",
    "        return sim_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c114c8a8-bf9a-4658-9a8f-4176d80495a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_arrow_table_from_foundry(foundry_client, dataset_rid, branch_name=None, columns=None):\n",
    "    \n",
    "    arrow_buffer = foundry_client.datasets.Dataset.read_table(\n",
    "        dataset_rid,\n",
    "        format='ARROW',\n",
    "        branch_name=branch_name,\n",
    "        columns=columns\n",
    "    )\n",
    "    buffer_reader = pa.BufferReader(arrow_buffer)\n",
    "    reader = pa.ipc.RecordBatchStreamReader(buffer_reader)\n",
    "\n",
    "    return reader.read_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dc7b007-6ec6-4155-9b2b-fd044a6ad7b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "node_status = pl.from_arrow(\n",
    "    load_arrow_table_from_foundry(\n",
    "        foundry_client=foundry_client,\n",
    "        dataset_rid=node_status_gdi_rid,\n",
    "        branch_name=\"master\",\n",
    "        columns=['node_id', 'timestamp', 'is_starved', 'is_blocked', 'is_deferred_bottleneck']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34de3ad8-5c81-46bf-a2e3-077fc0b90533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"POLARS_VERBOSE\"] = \"0\"\n",
    "pl.Config.set_fmt_str_lengths(1000)\n",
    "pl.Config.set_tbl_rows(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91e2e38f-09f4-4f23-b1f7-56656b3277ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_predicates(node_links, use_case_id = 'gudai_darri_vc'):\n",
    "    \"\"\"\n",
    "    Generates a list of predicates and corresponding column names based on parent-child \n",
    "    relationships in a given dataset.\n",
    "\n",
    "    This function processes a dataset of node links to determine dependencies between nodes \n",
    "    and constructs conditional expressions (predicates) using the Polars library. These predicates \n",
    "    help analyze whether nodes are blocked, starved, or in specific states based on their parents and children.\n",
    "\n",
    "    Args:\n",
    "        node_links (pl.DataFrame): A Polars DataFrame containing columns:\n",
    "            - 'parent_value_chain_node_id': The parent node identifier.\n",
    "            - 'child_value_chain_node_id': The child node identifier.\n",
    "            - 'use_case_id': The identifier for filtering the dataset.\n",
    "        use_case_id (str, optional): The identifier used to filter and process the dataset. \n",
    "            Defaults to 'gudai_darri_vc'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - list of str: Polars expressions defining conditions for each node (e.g., blocked, starved).\n",
    "            - list of str: Corresponding column names associated with the predicates.\n",
    "    \"\"\"\n",
    "\n",
    "    node_link_dict = (\n",
    "        node_links\n",
    "        .with_columns(\n",
    "            pl.col('parent_value_chain_node_id').str.replace(f'{use_case_id}_', '').alias('parent'),\n",
    "            pl.col('child_value_chain_node_id').str.replace(f'{use_case_id}_', '').alias('child'),\n",
    "        )\n",
    "        .filter(pl.col('use_case_id') == 'gudai_darri_vc')\n",
    "        .select(['parent', 'child'])\n",
    "    #    .collect()\n",
    "        .to_dicts()\n",
    "    )\n",
    "\n",
    "    all_nodes = list(set(itertools.chain.from_iterable([list(row.values()) for row in node_link_dict])))\n",
    "    \n",
    "    all_predicates = []\n",
    "    columns_to_select = []\n",
    "    for node in all_nodes:\n",
    "        children = [rel_dict['child'] for rel_dict in node_link_dict if rel_dict['parent'] == node]\n",
    "        parents = [rel_dict['parent'] for rel_dict in node_link_dict if rel_dict['child'] == node]\n",
    "\n",
    "        columns_to_select.append(\n",
    "            f\"is_deferred_bottleneck#{node}\"\n",
    "        )\n",
    "        \n",
    "        if len(parents) > 0:\n",
    "            all_parents_blocked_predicate = \"(\" + (\" & \").join([f\"pl.col('is_blocked#{parent}')\" for parent in parents]) + \")\"\n",
    "            all_parents_free_predicate = \"(\" + (\" & \").join([f\"~pl.col('is_blocked#{parent}')\" for parent in parents]) + \")\"\n",
    "            some_not_all_parents_blocked_predicate = \"(\" + \"(\" + (\" | \").join([f\"pl.col('is_blocked#{parent}')\" for parent in parents]) +\")\" + \" & \" + \"~\" + all_parents_blocked_predicate + \")\"\n",
    "            some_or_all_parents_free_predicate = \"(\" + some_not_all_parents_blocked_predicate + \" | \" + all_parents_free_predicate + \")\"\n",
    "        else:\n",
    "            all_parents_blocked_predicate = \"pl.lit(True)\"\n",
    "            all_parents_free_predicate = \"pl.lit(False)\"\n",
    "            some_not_all_parents_blocked_predicate = \"pl.lit(True)\"\n",
    "            some_or_all_parents_free_predicate = \"pl.lit(False)\"\n",
    "            \n",
    "        if len(children) > 0:\n",
    "            all_children_starved_predicate = \"(\" + (\" & \").join([f\"pl.col('is_starved#{child}')\" for child in children]) + \")\"\n",
    "            all_children_free_predicate = \"(\" + (\" & \").join([f\"~pl.col('is_starved#{child}')\" for child in children]) + \")\"\n",
    "            some_not_all_children_starved_predicate = \"(\" + \"(\" + (\" | \").join([f\"pl.col('is_starved#{child}')\" for child in children]) +\")\"  + \" & \" + \"~\" + all_children_starved_predicate + \")\"\n",
    "            some_or_all_children_free_predicate = \"(\" + some_not_all_children_starved_predicate + \" | \" + all_children_free_predicate + \")\"\n",
    "        else:\n",
    "            all_children_starved_predicate = \"pl.lit(True)\"\n",
    "            all_children_free_predicate = \"pl.lit(False)\"\n",
    "            some_not_all_children_starved_predicate = \"pl.lit(True)\"\n",
    "            some_or_all_children_free_predicate = \"pl.lit(False)\"\n",
    "\n",
    "        if len(parents) > 0 and len(children) > 0:\n",
    "            all_predicates.append(\n",
    "                f\"pl.when((pl.col('is_deferred_bottleneck#{node}') == pl.lit(1, dtype=pl.Int64)) & {all_parents_blocked_predicate} & {all_children_starved_predicate}).then(1).otherwise(0).alias('blocked_starved#{node}')\"\n",
    "            )\n",
    "            columns_to_select.append(\n",
    "                f'blocked_starved#{node}'\n",
    "            )\n",
    "\n",
    "        \n",
    "            all_predicates.append(\n",
    "                f\"pl.when((pl.col('is_deferred_bottleneck#{node}') == pl.lit(1, dtype=pl.Int64)) & {all_parents_free_predicate} & {all_children_free_predicate}).then(1).otherwise(0).alias('notblocked_notstarved#{node}')\"\n",
    "            )\n",
    "            columns_to_select.append(\n",
    "                f'notblocked_notstarved#{node}'\n",
    "            )\n",
    "        \n",
    "                \n",
    "        all_predicates.append(\n",
    "            f\"pl.when((pl.col('is_deferred_bottleneck#{node}') == pl.lit(1, dtype=pl.Int64)) & ({some_not_all_parents_blocked_predicate} | ({all_parents_blocked_predicate} & {all_children_free_predicate}))).then(1).otherwise(0).alias('blocked#{node}')\"\n",
    "        )\n",
    "\n",
    "        columns_to_select.append(\n",
    "            f'blocked#{node}'\n",
    "        )\n",
    "\n",
    "\n",
    "        all_predicates.append(\n",
    "            f\"pl.when((pl.col('is_deferred_bottleneck#{node}') == pl.lit(1, dtype=pl.Int64)) & ({some_not_all_children_starved_predicate} | ({all_children_starved_predicate} & {all_parents_free_predicate}))).then(1).otherwise(0).alias('starved#{node}')\"\n",
    "        )\n",
    "\n",
    "        columns_to_select.append(\n",
    "            f'starved#{node}'\n",
    "        )\n",
    "    \n",
    "    return all_predicates, columns_to_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "217d12c9-6bab-49ac-afea-2561ad6c1bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "node_status_raw = (\n",
    " node_status\n",
    "    .filter(\n",
    "        pl.col('timestamp').dt.date() == datetime(2025, 3, 5).date(),\n",
    "    )\n",
    "    # .collect()\n",
    "    .pivot(\n",
    "        on='node_id', \n",
    "        index='timestamp',\n",
    "        values=['is_starved', 'is_blocked', 'is_deferred_bottleneck'],\n",
    "        separator = '#'\n",
    "    )\n",
    "    .with_row_index(\"id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb98e694-7a0f-43b2-8967-c996a1d92535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_agg = (\n",
    "    node_status\n",
    "    # .collect()\n",
    "    .pivot(\n",
    "        on='node_id', \n",
    "        index='timestamp',\n",
    "        values=['is_starved', 'is_blocked', 'is_deferred_bottleneck'],\n",
    "        separator = '#'\n",
    "    ).with_columns(\n",
    "       [eval(predicate) for predicate in all_predicates]\n",
    "    ).select(['timestamp'] + columns_to_select)\n",
    "    .unpivot(\n",
    "        index='timestamp',\n",
    "        variable_name = 'attr_names',\n",
    "        value_name = 'attr_values'\n",
    "    ).with_columns(\n",
    "        pl.col(\"attr_names\").str.splitn(\"#\", 2).struct.rename_fields([\"attr_name\", \"node_id\"]).alias(\"fields\")\n",
    "    ).unnest(\"fields\")\n",
    "    .pivot(\n",
    "        on='attr_name', \n",
    "        index=['timestamp', 'node_id'],\n",
    "        values='attr_values'\n",
    "    ).with_columns(\n",
    "        pl.col('timestamp').dt.date().alias('date')\n",
    "    ).group_by(['date', 'node_id'])\n",
    "    .agg(\n",
    "        pl.col('is_deferred_bottleneck').sum(),\n",
    "        pl.col('blocked_starved').sum(),\n",
    "        pl.col('notblocked_notstarved').sum(),\n",
    "        pl.col('blocked').sum(),\n",
    "        pl.col('starved').sum()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5600a59-410b-4f9f-8176-018d105e16d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class DataTypeNotExists(Exception):\n",
    "    pass\n",
    "\n",
    "class OperatorNotExists(Exception):\n",
    "    pass\n",
    "\n",
    "def evaluate_rule_truth_values(dataset, rules):\n",
    "    \"\"\"\n",
    "    Evaluates the truth values of rules against a dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): A list of data rows, where each row is a dictionary containing data attributes.\n",
    "        rules (list): A list of rules, where each rule is a dictionary containing rule attributes such as\n",
    "                      'data_type', 'operator', 'min_value', 'max_value', 'evaluation_node', and 'metric_id'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - rule_truth_values (dict): A dictionary mapping rule IDs to their truth values (True/False).\n",
    "            - rule_values (dict): A dictionary mapping rule IDs to the evaluated metric values.\n",
    "            - rule_ref_values (dict): A dictionary mapping rule IDs to their reference values (min and max).\n",
    "            - rule_operator (dict): A dictionary mapping rule IDs to their operators.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = [row.asDict() for row in dataset]\n",
    "    rules = [\n",
    "        rule\n",
    "        for rule in rules\n",
    "        if rule['active_window_start'] <= dataset[0]['timestamp']\n",
    "        and rule['active_window_end'] > dataset[0]['timestamp']\n",
    "    ]\n",
    "\n",
    "    rule_truth_values = {}\n",
    "    rule_values = {}\n",
    "    rule_ref_values = {}\n",
    "    rule_operator = {}\n",
    "    for rule in rules:\n",
    "        try:\n",
    "            if rule['data_type'] == 'NUMERIC':\n",
    "                ref_val_min = float(rule['min_value'])\n",
    "                ref_val_max = float(rule['max_value'])\n",
    "                val_list = [\n",
    "                    data\n",
    "                    for data in dataset\n",
    "                    if data['node_id'] == rule['evaluation_node']\n",
    "                    and data['metric_id'] == rule['metric_id']\n",
    "                ]\n",
    "\n",
    "                if len(val_list) > 0:\n",
    "                    if val_list[0]['metric_value'] is not None:\n",
    "                        val = float(val_list[0]['metric_value'])\n",
    "                    else:\n",
    "                        val = None\n",
    "                else:\n",
    "                    val = None\n",
    "            elif rule['data_type'] == 'SET':\n",
    "                ref_val_min = rule['min_value'].strip().split(';')\n",
    "                ref_val_max = rule['max_value'].strip().split(';')\n",
    "                val_list = [\n",
    "                    data\n",
    "                    for data in dataset\n",
    "                    if data['node_id'] == rule['evaluation_node']\n",
    "                    and data['metric_id'] == rule['metric_id']\n",
    "                ]\n",
    "\n",
    "                if len(val_list) > 0:\n",
    "                    if val_list[0]['metric_value'] is not None:\n",
    "                        val = val_list[0]['metric_value']\n",
    "                    else:\n",
    "                        val = None\n",
    "                else:\n",
    "                    val = None\n",
    "            elif rule['data_type'] == 'TEXT':\n",
    "                ref_val_min = rule['min_value']\n",
    "                ref_val_max = rule['max_value']\n",
    "                val_list = [\n",
    "                    data\n",
    "                    for data in dataset\n",
    "                    if data['node_id'] == rule['evaluation_node']\n",
    "                    and data['metric_id'] == rule['metric_id']\n",
    "                ]\n",
    "\n",
    "                if len(val_list) > 0:\n",
    "                    if val_list[0]['metric_value'] is not None:\n",
    "                        val = val_list[0]['metric_value']\n",
    "                    else:\n",
    "                        val = None\n",
    "                else:\n",
    "                    val = None\n",
    "            else:\n",
    "                raise DataTypeNotExists(\n",
    "                    f\"{rule['data_type']} type doesn't exist. Allowed types TEXT, NUMERIC, SET\"\n",
    "                )\n",
    "        except ValueError:\n",
    "            raise ValueError(\n",
    "                f\"\"\"Error: Cannot convert '{rule['min_value']}' or '{rule['min_value']}'\n",
    "                    on dataset: '{rule['dataset']}' and metric_id: '{rule['metric_id']}' \"\"\"\n",
    "            )\n",
    "\n",
    "        if val is not None:\n",
    "            if rule['operator'] == 'RANGE':\n",
    "                truth_val = val >= ref_val_min and val <= ref_val_max\n",
    "            elif rule['operator'] == 'GTE':\n",
    "                truth_val = val >= ref_val_min\n",
    "            elif rule['operator'] == 'LTE':\n",
    "                truth_val = val <= ref_val_min\n",
    "            elif rule['operator'] == 'GT':\n",
    "                truth_val = val > ref_val_min\n",
    "            elif rule['operator'] == 'LT':\n",
    "                truth_val = val < ref_val_min\n",
    "            elif rule['operator'] == 'EQUAL':\n",
    "                truth_val = val == ref_val_min\n",
    "            elif rule['operator'] == 'NOT EQUAL':\n",
    "                truth_val = val != ref_val_min\n",
    "            elif rule['operator'] == 'STARTS_WITH':\n",
    "                truth_val = val.startswith(ref_val_min)\n",
    "            elif rule['operator'] == 'NOT STARTS_WITH':\n",
    "                truth_val = not val.startswith(ref_val_min)\n",
    "            elif rule['operator'] == 'ENDS_WITH':\n",
    "                truth_val = val.endswith(ref_val_min)\n",
    "            elif rule['operator'] == 'NOT ENDS_WITH':\n",
    "                truth_val = not val.endswith(ref_val_min)\n",
    "            elif rule['operator'] == 'CONTAINS':\n",
    "                truth_val = ref_val_min in val\n",
    "            elif rule['operator'] == 'NOT CONTAINS':\n",
    "                truth_val = ref_val_min not in val\n",
    "            elif rule['operator'] == 'IN':\n",
    "                truth_val = val in ref_val_min\n",
    "            elif rule['operator'] == 'NOT IN':\n",
    "                truth_val = val not in ref_val_min\n",
    "            else:\n",
    "                raise OperatorNotExists(\n",
    "                    f\"\"\"{rule['operator']} operator doesn't exist. Allowed operators\n",
    "                        RANGE, GTE, LTE, GT, LT, EQUAL, NOT EQUAL, STARTS_WITH, NOT STARTS_WITH,\n",
    "                        ENDS_WITH, NOT ENDS_WITH, CONTAINS, NOT CONTAINS, IN, NOT IN\"\"\"\n",
    "                )\n",
    "        else:\n",
    "            truth_val = False\n",
    "\n",
    "        rule_truth_values[rule['rule_id']] = truth_val\n",
    "        rule_values[rule['rule_id']] = val\n",
    "        rule_ref_values[rule['rule_id']] = {'ref_val_min': ref_val_min, 'ref_val_max': ref_val_max}\n",
    "        rule_operator[rule['rule_id']] = rule['operator']\n",
    "    \n",
    "    return rule_truth_values, rule_values, rule_ref_values, rule_operator\n",
    "\n",
    "def evaluate_node_rules(rule_truth_values, rule_values, rule_ref_values, rule_operator, node_config_dict, rules):\n",
    "    \"\"\"\n",
    "    Evaluates rules for each node and determines their states (e.g., BLOCKED, STARVED).\n",
    "\n",
    "    Args:\n",
    "        rule_truth_values (dict): A dictionary mapping rule IDs to their truth values.\n",
    "        rule_values (dict): A dictionary mapping rule IDs to their evaluated metric values.\n",
    "        rule_ref_values (dict): A dictionary mapping rule IDs to their reference values.\n",
    "        rule_operator (dict): A dictionary mapping rule IDs to their operators.\n",
    "        node_config_dict (list): A list of dictionaries containing node configuration details.\n",
    "        rules (list): A list of rules, where each rule is a dictionary containing rule attributes.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains the evaluation results for a node.\n",
    "    \"\"\"\n",
    "\n",
    "    for i, j in rule_truth_values.items():\n",
    "        exec(f\"{i} = {j}\")\n",
    "\n",
    "    all_nodes_rules = {(rule['node'], rule['rule_type']) for rule in rules}\n",
    "\n",
    "    all_nodes = {\n",
    "        node['parent_node_id'] for node in node_config_dict} | {node['child_node_id'] for node in node_config_dict\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for node_id in all_nodes:\n",
    "        node_level_result = {}\n",
    "        node_level_result['node_id'] = node_id\n",
    "        for rule_type in ['BLOCKED', 'STARVED']:\n",
    "            if (node_id, rule_type) in all_nodes_rules:\n",
    "                truth_exp = [\n",
    "                    rule['truth_value']\n",
    "                    for rule in rules\n",
    "                    if rule['node'] == node_id\n",
    "                    and rule['rule_type'] == rule_type\n",
    "                ][0]\n",
    "\n",
    "                rules_to_save = [\n",
    "                    rule['rule_id']\n",
    "                    for rule in rules\n",
    "                    if rule['node'] == node_id\n",
    "                    and rule['rule_type'] == rule_type\n",
    "                ]\n",
    "\n",
    "                node_level_result[rule_type] = eval(truth_exp)\n",
    "\n",
    "                node_level_result[rule_type + '_RULES'] = str(\n",
    "                    {i[0]: i[1] for i in rule_truth_values.items() if i[0] in rules_to_save}\n",
    "                )\n",
    "\n",
    "                node_level_result[rule_type + '_OPERATORS'] = str(\n",
    "                    {i[0]: i[1] for i in rule_operator.items() if i[0] in rules_to_save}\n",
    "                )\n",
    "\n",
    "                node_level_result[rule_type + '_VALUES'] = str(\n",
    "                    {i[0]: i[1] for i in rule_values.items() if i[0] in rules_to_save}\n",
    "                )\n",
    "\n",
    "                node_level_result[rule_type + '_REF_VALUES'] = str(\n",
    "                    {i[0]: i[1] for i in rule_ref_values.items() if i[0] in rules_to_save}\n",
    "                )\n",
    "\n",
    "                node_level_result[rule_type + '_RULE_DESC'] = [\n",
    "                    {\n",
    "                        'rule_id': rule['rule_id'],\n",
    "                        'long_rule_desc': rule['rule_explanation'],\n",
    "                        'short_rule_desc': rule['short_rule_desc']\n",
    "                    }\n",
    "                    for rule in rules\n",
    "                    if (\n",
    "                        rule['node'] == node_id\n",
    "                        and rule['rule_type'] == rule_type\n",
    "                        and rule_truth_values[rule['rule_id']]\n",
    "                    )\n",
    "                ]\n",
    "                node_level_result[rule_type + '_TRUTH'] = truth_exp\n",
    "            else:\n",
    "                node_level_result[rule_type] = False\n",
    "                node_level_result[rule_type + '_RULES'] = None\n",
    "                node_level_result[rule_type + '_OPERATORS'] = None\n",
    "                node_level_result[rule_type + '_VALUES'] = None\n",
    "                node_level_result[rule_type + '_REF_VALUES'] = None\n",
    "                node_level_result[rule_type + '_RULE_DESC'] = [\n",
    "                    {'rule_id': None, 'long_rule_desc': None, 'short_rule_desc': None}\n",
    "                ]\n",
    "                node_level_result[rule_type + '_TRUTH'] = None\n",
    "\n",
    "        if node_level_result['BLOCKED'] and node_level_result['STARVED']:\n",
    "            node_level_result['BLOCKED'] = False\n",
    "\n",
    "        results.append(node_level_result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def resolve_state_conflict(results, node_config_dict):\n",
    "    \"\"\"\n",
    "    Resolves state conflicts between parent and child nodes based on their activity and state.\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of dictionaries containing node evaluation results.\n",
    "        node_config_dict (list): A list of dictionaries containing node configuration details.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries with updated node states after resolving conflicts.\n",
    "    \"\"\"\n",
    "\n",
    "    inactive_parents = deepcopy([node for node in node_config_dict if node['is_parent_active'] == 0])\n",
    "    inactive_children = deepcopy([node for node in node_config_dict if node['is_child_active'] == 0])\n",
    "\n",
    "    for inactive_node in inactive_parents:\n",
    "        parent_idx = [i for i, j in enumerate(results) if j['node_id'] == inactive_node['parent_node_id']][0]\n",
    "        parent_node = results.pop(parent_idx)\n",
    "\n",
    "        child_idx = [i for i, j in enumerate(results) if j['node_id'] == inactive_node['child_node_id']][0]\n",
    "        child_node = results.pop(child_idx)\n",
    "\n",
    "        if child_node['STARVED']:\n",
    "            parent_node['STARVED'] = False\n",
    "            parent_node['BLOCKED'] = False\n",
    "        else:\n",
    "            parent_node['BLOCKED'] = True\n",
    "            parent_node['STARVED'] = False\n",
    "\n",
    "        results.append(child_node)\n",
    "        results.append(parent_node)\n",
    "\n",
    "    for inactive_node in inactive_children:\n",
    "        parent_idx = [i for i, j in enumerate(results) if j['node_id'] == inactive_node['parent_node_id']][0]\n",
    "        parent_node = results.pop(parent_idx)\n",
    "\n",
    "        child_idx = [i for i, j in enumerate(results) if j['node_id'] == inactive_node['child_node_id']][0]\n",
    "        child_node = results.pop(child_idx)\n",
    "\n",
    "        if parent_node['BLOCKED']:\n",
    "            child_node['STARVED'] = False\n",
    "            child_node['BLOCKED'] = False\n",
    "        else:\n",
    "            child_node['BLOCKED'] = False\n",
    "            child_node['STARVED'] = True\n",
    "\n",
    "        results.append(child_node)\n",
    "        results.append(parent_node)\n",
    "\n",
    "    return results\n",
    "\n",
    "def identify_bottlenecks(results, branches):\n",
    "    \"\"\"\n",
    "    Identifies bottlenecks in a value chain by analyzing node states and their relationships.\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of dictionaries containing node evaluation results.\n",
    "        branches (list): A list of branch paths, where each branch is a dictionary containing\n",
    "                         path details such as 'path_array' and 'path_weight'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries with updated node states, including bottleneck identification.\n",
    "    \"\"\"\n",
    "\n",
    "    original_node_states = {}\n",
    "    for branch in branches:\n",
    "        branch = branch.asDict()\n",
    "        is_bottleneck_found = False\n",
    "        processed = set()\n",
    "        for parent_node_id, child_node_id in list(zip(branch['path_array'][-2::-1], branch['path_array'][::-1][:-1])):\n",
    "\n",
    "            if parent_node_id not in original_node_states.keys():\n",
    "                parent_node_idx = [\n",
    "                    i for i, j in enumerate(results)\n",
    "                    if j['node_id'] == parent_node_id\n",
    "                ][0]\n",
    "\n",
    "                parent_node_result_original = results.pop(parent_node_idx)\n",
    "                parent_node_result = deepcopy(parent_node_result_original)\n",
    "                original_node_states[parent_node_id] = deepcopy(parent_node_result_original)\n",
    "                processed.add(parent_node_id)\n",
    "            elif parent_node_id in processed:\n",
    "                parent_node_idx = [\n",
    "                    i for i, j in enumerate(results)\n",
    "                    if (j['node_id'] == parent_node_id)\n",
    "                    & (j.get('vc_path_id', 'empty') == branch['path_id'])\n",
    "                ][0]\n",
    "\n",
    "                parent_node_result_original = results.pop(parent_node_idx)\n",
    "                parent_node_result = deepcopy(parent_node_result_original)\n",
    "            else:\n",
    "                parent_node_result = deepcopy(original_node_states[parent_node_id])\n",
    "                processed.add(parent_node_id)\n",
    "\n",
    "            if child_node_id not in original_node_states.keys():\n",
    "                child_node_idx = [i for i, j in enumerate(results) if j['node_id'] == child_node_id][0]\n",
    "                child_node_result_original = results.pop(child_node_idx)\n",
    "                child_node_result = deepcopy(child_node_result_original)\n",
    "                original_node_states[child_node_id] = deepcopy(child_node_result_original)\n",
    "                processed.add(child_node_id)\n",
    "            elif child_node_id in processed:\n",
    "                child_node_idx = [\n",
    "                    i for i, j in enumerate(results)\n",
    "                    if (j['node_id'] == child_node_id)\n",
    "                    & (j.get('vc_path_id', 'empty') == branch['path_id'])\n",
    "                ][0]\n",
    "\n",
    "                child_node_result_original = results.pop(child_node_idx)\n",
    "                child_node_result = deepcopy(child_node_result_original)\n",
    "            else:\n",
    "                child_node_result = deepcopy(original_node_states[child_node_id])\n",
    "                processed.add(child_node_id)\n",
    "\n",
    "            parent_node_result['vc_path_id'] = branch['path_id']\n",
    "            parent_node_result['vc_path_weight'] = branch['path_weight']\n",
    "            parent_node_result['is_bottleneck'] = parent_node_result.get('is_bottleneck', False)\n",
    "            parent_node_result['starving_downstream'] = parent_node_result.get('starving_downstream', False)\n",
    "            parent_node_result['blocking_upstream'] = parent_node_result.get('blocking_upstream', False)\n",
    "            parent_node_result['bottleneck_desc'] = parent_node_result.get(\n",
    "                'bottleneck_desc',\n",
    "                [{'rule_id': None, 'long_rule_desc': None, 'short_rule_desc': None}]\n",
    "            )\n",
    "\n",
    "            child_node_result['vc_path_id'] = branch['path_id']\n",
    "            child_node_result['vc_path_weight'] = branch['path_weight']\n",
    "            child_node_result['is_bottleneck'] = child_node_result.get('is_bottleneck', False)\n",
    "            child_node_result['starving_downstream'] = child_node_result.get('starving_downstream', False)\n",
    "            child_node_result['blocking_upstream'] = child_node_result.get('blocking_upstream', False)\n",
    "            child_node_result['bottleneck_desc'] = child_node_result.get(\n",
    "                'bottleneck_desc',\n",
    "                [{'rule_id': None, 'long_rule_desc': None, 'short_rule_desc': None}]\n",
    "            )\n",
    "\n",
    "            if parent_node_result['BLOCKED'] and child_node_result['STARVED']:\n",
    "                parent_node_result['BLOCKED'] = False\n",
    "\n",
    "            if child_node_result['STARVED']:\n",
    "                parent_node_result['starving_downstream'] = True\n",
    "                if parent_node_result['bottleneck_desc'] == [\n",
    "                    {'rule_id': None, 'long_rule_desc': None, 'short_rule_desc': None\n",
    "                }]:\n",
    "                    parent_node_result['bottleneck_desc'] = child_node_result['STARVED_RULE_DESC']\n",
    "                else:\n",
    "                    parent_node_result['bottleneck_desc'] += child_node_result['STARVED_RULE_DESC']\n",
    "\n",
    "            if parent_node_result['BLOCKED']:\n",
    "                child_node_result['blocking_upstream'] = True\n",
    "                if child_node_result['bottleneck_desc'] == [\n",
    "                    {'rule_id': None, 'long_rule_desc': None, 'short_rule_desc': None\n",
    "                }]:\n",
    "                    child_node_result['bottleneck_desc'] = parent_node_result['BLOCKED_RULE_DESC']\n",
    "                else:\n",
    "                    child_node_result['bottleneck_desc'] += parent_node_result['BLOCKED_RULE_DESC']\n",
    "\n",
    "            if not is_bottleneck_found:\n",
    "                if parent_node_result['starving_downstream'] and not parent_node_result['STARVED']:\n",
    "                    parent_node_result['is_bottleneck'] = True\n",
    "                    is_bottleneck_found = True\n",
    "                elif parent_node_result['blocking_upstream'] and not parent_node_result['BLOCKED']:\n",
    "                    parent_node_result['is_bottleneck'] = True\n",
    "                    is_bottleneck_found = True\n",
    "                elif child_node_result['starving_downstream'] and not child_node_result['STARVED']:\n",
    "                    child_node_result['is_bottleneck'] = True\n",
    "                    is_bottleneck_found = True\n",
    "                elif child_node_result['blocking_upstream'] and not child_node_result['BLOCKED']:\n",
    "                    child_node_result['is_bottleneck'] = True\n",
    "                    is_bottleneck_found = True\n",
    "            results.append(parent_node_result)\n",
    "            results.append(child_node_result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_and_identify_bottlenecks(dataset, rules, node_config_dict, branches):\n",
    "    \"\"\"\n",
    "    Evaluates rules against a dataset, resolves state conflicts, and identifies bottlenecks in a value chain.\n",
    "\n",
    "    This function combines the evaluation of truth values for rules, the resolution of state conflicts \n",
    "    between parent and child nodes, and the identification of bottlenecks in the value chain.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): A list of data rows, where each row is a dictionary containing data attributes.\n",
    "        rules (list): A list of rules, where each rule is a dictionary containing rule attributes such as\n",
    "                      'data_type', 'operator', 'min_value', 'max_value', 'evaluation_node', and 'metric_id'.\n",
    "        node_config_dict (list): A list of dictionaries containing node configuration details.\n",
    "        branches (list): A list of branch paths, where each branch is a dictionary containing\n",
    "                         path details such as 'path_array' and 'path_weight'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the evaluation results for nodes, including bottleneck identification.\n",
    "    \"\"\"\n",
    "\n",
    "    rule_truth_values, rule_values, rule_ref_values, rule_operator = evaluate_rule_truth_values(dataset, rules)\n",
    "    results = evaluate_node_rules(rule_truth_values, rule_values, rule_ref_values, rule_operator, node_config_dict, rules)\n",
    "    results = resolve_state_conflict(results, node_config_dict)\n",
    "\n",
    "    return identify_bottlenecks(results, branches)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "demo-notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
